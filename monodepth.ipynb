{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIRcr6J2agtQGAchybLkcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suarim/samsung_prism_camera_motion_estimation/blob/main/monodepth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uun4LZhAT19K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ZeroPadding2D, ReLU, MaxPooling2D\n",
        "\n",
        "\n",
        "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        self.input_spec = [tf.keras.layers.InputSpec(ndim=4)]\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, s):\n",
        "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        w_pad, h_pad = self.padding\n",
        "        return tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        return config\n",
        "\n",
        "\n",
        "def res_block(inputs, layer, downsample=False):\n",
        "    filters = inputs.shape[-1]\n",
        "    filters *= 2 if downsample else 1\n",
        "    strides = 2 if downsample else 1\n",
        "    pad1 = ZeroPadding2D(1)(inputs)\n",
        "    name = 'en.layer' + str(layer[0]) + '.' + str(layer[1]) + '.'\n",
        "    conv1 = Conv2D(filters, 3, activation='linear', use_bias=False, strides=strides,name=name + 'conv1')(pad1)\n",
        "    bn1 = BatchNormalization(momentum=0.9, epsilon=1e-5, name=name + 'bn1')(conv1)\n",
        "    relu1 = ReLU()(bn1)\n",
        "    pad2 = ZeroPadding2D(1)(relu1)\n",
        "    conv2 = Conv2D(filters, 3, activation='linear', use_bias=False,name=name + 'conv2')(pad2)\n",
        "    bn2 = BatchNormalization(momentum=0.9, epsilon=1e-5, name=name + 'bn2')(conv2)\n",
        "\n",
        "    if not downsample:\n",
        "        add = bn2 + inputs\n",
        "    else:\n",
        "        name += 'downsample.'\n",
        "        conv3 = Conv2D(filters, 1, activation='linear',use_bias=False, strides=2, name=name + '0')(inputs)\n",
        "        bn3 = BatchNormalization(momentum=0.9, epsilon=1e-5, name=name + '1')(conv3)\n",
        "        add = bn2 + bn3\n",
        "\n",
        "    relu2 = ReLU()(add)\n",
        "    return relu2\n",
        "\n",
        "\n",
        "def conv_block(size, inTensor, disp=False, cnt=''):\n",
        "    name = 'dispconv' if disp else 'upconv'\n",
        "    name = 'de.' + name + '.' + str(len(\"{0:b}\".format(size)) - 5) + '.' + cnt\n",
        "    filters = 1 if disp else size\n",
        "    x = ReflectionPadding2D()(inTensor)\n",
        "    x = tf.keras.layers.Conv2D(filters, 3, name=name)(x)\n",
        "    if not disp:\n",
        "        x = tf.keras.layers.ELU()(x)\n",
        "    else:\n",
        "        x = tf.keras.activations.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def up_conv(size, firstTensor, secondTensor=None):\n",
        "    x = conv_block(size, firstTensor, cnt='0')\n",
        "    x = tf.keras.layers.UpSampling2D()(x)\n",
        "    if size > 16:\n",
        "        x = tf.keras.layers.concatenate([x, secondTensor], axis=-1)\n",
        "    x = conv_block(size, x, cnt='1')\n",
        "    return x"
      ],
      "metadata": {
        "id": "CudIdURBUEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "def download_model_if_doesnt_exist(model_name):\n",
        "    \"\"\"If pretrained kitti model doesn't exist, download and unzip it\n",
        "    \"\"\"\n",
        "    # values are tuples of (<google cloud URL>, <md5 checksum>)\n",
        "    download_paths = {\n",
        "        \"mono_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip\",\n",
        "             \"a964b8356e08a02d009609d9e3928f7c\"),\n",
        "        \"stereo_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip\",\n",
        "             \"3dfb76bcff0786e4ec07ac00f658dd07\"),\n",
        "        \"mono+stereo_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip\",\n",
        "             \"c024d69012485ed05d7eaa9617a96b81\"),\n",
        "        \"mono_no_pt_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip\",\n",
        "             \"9c2f071e35027c895a4728358ffc913a\"),\n",
        "        \"stereo_no_pt_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip\",\n",
        "             \"41ec2de112905f85541ac33a854742d1\"),\n",
        "        \"mono+stereo_no_pt_640x192\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip\",\n",
        "             \"46c3b824f541d143a45c37df65fbab0a\"),\n",
        "        \"mono_1024x320\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip\",\n",
        "             \"0ab0766efdfeea89a0d9ea8ba90e1e63\"),\n",
        "        \"stereo_1024x320\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip\",\n",
        "             \"afc2f2126d70cf3fdf26b550898b501a\"),\n",
        "        \"mono+stereo_1024x320\":\n",
        "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip\",\n",
        "             \"cdc5fc9b23513c07d5b19235d9ef08f7\"),\n",
        "        }\n",
        "\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "\n",
        "    model_path = os.path.join(\"models\", model_name)\n",
        "\n",
        "    def check_file_matches_md5(checksum, fpath):\n",
        "        if not os.path.exists(fpath):\n",
        "            return False\n",
        "        with open(fpath, 'rb') as f:\n",
        "            current_md5checksum = hashlib.md5(f.read()).hexdigest()\n",
        "        return current_md5checksum == checksum\n",
        "\n",
        "    # see if we have the model already downloaded...\n",
        "    if not os.path.exists(os.path.join(model_path, \"encoder.pth\")):\n",
        "\n",
        "        model_url, required_md5checksum = download_paths[model_name]\n",
        "\n",
        "        if not check_file_matches_md5(required_md5checksum, model_path + \".zip\"):\n",
        "            print(\"-> Downloading pretrained model to {}\".format(model_path + \".zip\"))\n",
        "            urllib.request.urlretrieve(model_url, model_path + \".zip\")\n",
        "\n",
        "        if not check_file_matches_md5(required_md5checksum, model_path + \".zip\"):\n",
        "            print(\"   Failed to download a file which matches the checksum - quitting\")\n",
        "            quit()\n",
        "\n",
        "        print(\"   Unzipping model...\")\n",
        "        with zipfile.ZipFile(model_path + \".zip\", 'r') as f:\n",
        "            f.extractall(model_path)\n",
        "\n",
        "        print(\"   Model unzipped to {}\".format(model_path))"
      ],
      "metadata": {
        "id": "lYa-BvcqUPA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mono_640x192\"\n",
        "\n",
        "download_model_if_doesnt_exist(model_name)\n",
        "encoder_path = os.path.join(\"models\", model_name, \"encoder.pth\")\n",
        "depth_decoder_path = os.path.join(\"models\", model_name, \"depth.pth\")"
      ],
      "metadata": {
        "id": "JbfbkI9RUPu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(192, 640, 3))\n",
        "encoder = []\n",
        "outputs = []\n",
        "\n",
        "# Encoder part:\n",
        "x = (inputs - 0.45) / 0.225\n",
        "x = ZeroPadding2D(3)(x)\n",
        "x = Conv2D(64, 7, strides=2, activation='linear', use_bias=False, name='conv1')(x)\n",
        "x = BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n",
        "x = ReLU()(x)\n",
        "encoder.append(x)\n",
        "x = ZeroPadding2D(1)(x)\n",
        "x = MaxPooling2D(3, 2)(x)\n",
        "\n",
        "for i in range(1, 5):\n",
        "    x = res_block(x, (i, 0), i > 1)\n",
        "    x = res_block(x, (i, 1))\n",
        "    encoder.append(x)\n",
        "\n",
        "\n",
        "# Decoder part:\n",
        "x = up_conv(256, encoder[4], encoder[3])\n",
        "x = up_conv(128, x, encoder[2])\n",
        "outputs.append(conv_block(128, x, disp=True))\n",
        "\n",
        "x = up_conv(64, x, encoder[1])\n",
        "outputs.append(conv_block(64, x, disp=True))\n",
        "\n",
        "x = up_conv(32, x, encoder[0])\n",
        "outputs.append(conv_block(32, x, disp=True))\n",
        "\n",
        "x = up_conv(16, x)\n",
        "outputs.append(conv_block(16, x, disp=True))\n",
        "\n",
        "outputs = outputs[::-1]\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='depth')\n",
        "\n",
        "\n",
        "encoder_path = 'models/mono_640x192/encoder.pth'\n",
        "decoder_path = 'models/mono_640x192/depth.pth'\n",
        "\n",
        "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
        "loaded_dict = torch.load(decoder_path, map_location='cpu')\n",
        "\n",
        "model.get_layer('conv1').set_weights(\n",
        "    [loaded_dict_enc['encoder.conv1.weight'].numpy().transpose(2, 3, 1, 0)])\n",
        "\n",
        "model.get_layer('bn1').set_weights(\n",
        "    [loaded_dict_enc['encoder.bn1.weight'].numpy(),\n",
        "     loaded_dict_enc['encoder.bn1.bias'].numpy(),\n",
        "     loaded_dict_enc['encoder.bn1.running_mean'].numpy(),\n",
        "     loaded_dict_enc['encoder.bn1.running_var'].numpy()])\n",
        "\n",
        "for layer in model.layers:\n",
        "    name = layer.name.split('.')\n",
        "    if name[0] == 'en':\n",
        "        name = '.'.join(name[1:])\n",
        "        num_weights = len(layer.get_weights())\n",
        "        if num_weights == 1:\n",
        "            layer.set_weights([loaded_dict_enc['encoder.' + name + '.weight'].numpy().transpose(2, 3, 1, 0)])\n",
        "        else:\n",
        "            layer.set_weights(\n",
        "                [loaded_dict_enc['encoder.' + name + '.weight'].numpy(),\n",
        "                 loaded_dict_enc['encoder.' + name + '.bias'].numpy(),\n",
        "                 loaded_dict_enc['encoder.' + name + '.running_mean'].numpy(),\n",
        "                 loaded_dict_enc['encoder.' + name + '.running_var'].numpy()])\n",
        "\n",
        "    if name[0] == 'de':\n",
        "        if name[1] == 'upconv':\n",
        "            num = str(2 * (4 - int(name[2])) + int(name[3]))\n",
        "            layer.set_weights([loaded_dict['decoder.' + num + '.conv.conv.weight'].numpy().transpose(2, 3, 1, 0),loaded_dict['decoder.' + num + '.conv.conv.bias'].numpy()])\n",
        "        else:\n",
        "            num = str(int(name[2]) + 10)\n",
        "            layer.set_weights([loaded_dict['decoder.' + num + '.conv.weight'].numpy().transpose(2, 3, 1, 0),loaded_dict['decoder.' + num + '.conv.bias'].numpy()])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-XtAzHYKUR4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import glob\n",
        "import os\n",
        "import PIL.Image as pil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "from PIL import Image as pil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "class CustomSmoothnessLoss(Loss):\n",
        "    def __init__(self, lambda_smooth=0.1):\n",
        "        super(CustomSmoothnessLoss, self).__init__()\n",
        "        self.lambda_smooth = lambda_smooth\n",
        "\n",
        "    def gradient(self, pred):\n",
        "        D_dy = pred[:, 1:, :, :] - pred[:, :-1, :, :]\n",
        "        D_dx = pred[:, :, 1:, :] - pred[:, :, :-1, :]\n",
        "        return D_dx, D_dy\n",
        "\n",
        "    def second_order_gradient(self, pred):\n",
        "        dx, dy = self.gradient(pred)\n",
        "        dx2, dxdy = self.gradient(dx)\n",
        "        dydx, dy2 = self.gradient(dy)\n",
        "        return dx2, dxdy, dydx, dy2\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Assuming y_pred is the predicted disparity map\n",
        "        pred_disp = y_pred\n",
        "\n",
        "        dx2, dxdy, dydx, dy2 = self.second_order_gradient(pred_disp)\n",
        "\n",
        "        smooth_loss = tf.reduce_mean(tf.abs(dx2)) + \\\n",
        "                      tf.reduce_mean(tf.abs(dxdy)) + \\\n",
        "                      tf.reduce_mean(tf.abs(dydx)) + \\\n",
        "                      tf.reduce_mean(tf.abs(dy2))\n",
        "\n",
        "        total_loss = self.lambda_smooth * smooth_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(192, 640)):\n",
        "    img = pil.open(image_path).convert('RGB')\n",
        "    img_resized = img.resize(target_size, pil.LANCZOS)\n",
        "    img_array = tf.convert_to_tensor(img_resized, dtype=tf.float32)\n",
        "    img_array = tf.expand_dims(img_array, axis=0)\n",
        "    return img_array / 255.0, img_array / 255.0\n",
        "\n",
        "\n",
        "\n",
        "def data_generator(dataset_loc, batch_size=8, target_size=(192, 640)):\n",
        "    while True:\n",
        "        batch_paths = np.random.choice(dataset_loc, size=batch_size)\n",
        "        batch = [load_and_preprocess_image(path, target_size=target_size) for path in batch_paths]\n",
        "        batch_images, target_images = zip(*batch)\n",
        "        batch_images = tf.concat(batch_images, axis=0)\n",
        "        target_images = tf.concat(target_images, axis=0)\n",
        "        yield batch_images, target_images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def depth_loss(predicted_depth1, predicted_depth2):\n",
        "#     # Concatenate predicted depth maps along the last axis (-1)\n",
        "#     predicted_depth1 = tf.concat(predicted_depth1, axis=-1)\n",
        "#     predicted_depth2 = tf.concat(predicted_depth2, axis=-1)\n",
        "\n",
        "#     # Resize the second predicted depth map to match the size of the first one\n",
        "#     predicted_depth2 = tf.image.resize(predicted_depth2, size=tf.shape(predicted_depth1)[1:3])\n",
        "\n",
        "#     # Compute reconstruction loss as the mean squared error\n",
        "#     reconstruction_loss = tf.reduce_mean(tf.square(predicted_depth1 - predicted_depth2))\n",
        "\n",
        "#     return reconstruction_loss\n",
        "# def depth_loss(true_depth, predicted_depth):\n",
        "#     predicted_depth = tf.concat(predicted_depth, axis=-1)\n",
        "\n",
        "#     predicted_depth = tf.image.resize(predicted_depth, size=tf.shape(true_depth)[1:3])\n",
        "\n",
        "#     mask = tf.math.logical_not(tf.math.is_nan(true_depth))\n",
        "\n",
        "#     reconstruction_loss = tf.reduce_sum(tf.square(tf.boolean_mask(true_depth - predicted_depth, mask)))\n",
        "\n",
        "#     num_valid_pixels = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
        "#     reconstruction_loss = tf.cond(tf.greater(num_valid_pixels, 0),\n",
        "#                                   lambda: reconstruction_loss / num_valid_pixels,\n",
        "#                                   lambda: tf.constant(0.0, dtype=tf.float32))\n",
        "\n",
        "#     return reconstruction_loss\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint('best_weights.h5', save_best_only=True, save_weights_only=True)\n",
        "\n",
        "\n",
        "\n",
        "dataset_loc = []\n",
        "data = ['2011_09_26_drive_0001'\n",
        "   ]\n",
        "\n",
        "\n",
        "\n",
        "for j in data:\n",
        "    for i in range(4):\n",
        "        base_path_template = f\"/content/drive/MyDrive/complete/2011_09_26/{j}_sync/image_0{i}/data\"\n",
        "        specific_path = os.path.join(base_path_template)\n",
        "        image_files = glob.glob(os.path.join(specific_path, \"*.png\"))\n",
        "        dataset_loc.extend(image_files)\n",
        "\n",
        "batch_size = 8\n",
        "epochs =3\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=CustomSmoothnessLoss(lambda_smooth=0.1))\n",
        "train_steps_per_epoch = (len(dataset_loc) / batch_size)\n",
        "train_generator = data_generator(dataset_loc, batch_size=batch_size)\n",
        "model.fit_generator(\n",
        "    generator=train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "ermMTV5fUZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL.Image as pil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "def test_function(image_path, loaded_dict_enc):\n",
        "    input_image = pil.open(image_path).convert('RGB')\n",
        "    original_width, original_height = input_image.size\n",
        "\n",
        "    feed_height = loaded_dict_enc['height']\n",
        "    feed_width = loaded_dict_enc['width']\n",
        "    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
        "\n",
        "    input_image_tf = tf.convert_to_tensor(input_image_resized, dtype=tf.float32)\n",
        "    input_image_tf = tf.expand_dims(input_image_tf, axis=0)\n",
        "\n",
        "    return input_image_tf\n",
        "\n",
        "image_path = r\"/content/drive/MyDrive/complete/2011_09_26/2011_09_26_drive_0059_sync/image_00/data/0000000000.png\"\n",
        "\n",
        "start_time = time.time()\n",
        "input_image_tf = test_function(image_path, loaded_dict_enc)\n",
        "end_time = time.time()\n",
        "\n",
        "processing_time = (end_time - start_time) * 1000\n",
        "print(f\"Processing time: {processing_time} milliseconds\")"
      ],
      "metadata": {
        "id": "xopnSO2xUi96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.predict(input_image_tf)"
      ],
      "metadata": {
        "id": "-DGWa2-7UnbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "tX81ibmSUplR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming res is a list of NumPy arrays\n",
        "\n",
        "# Access individual arrays directly\n",
        "disparity_map = res[0][0, :, :, 0]\n",
        "depth_map = 1 / (disparity_map + 1e-6)\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Disparity Map\n",
        "axs[0].imshow(disparity_map, cmap='plasma')\n",
        "axs[0].set_title('Disparity Map')\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Depth Map\n",
        "im = axs[1].imshow(depth_map, cmap='viridis')\n",
        "axs[1].set_title('Depth Map')\n",
        "axs[1].axis('off')\n",
        "fig.colorbar(im, ax=axs[1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Acsc48T-UqL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given strings\n",
        "strings = \"2011_09_26_drive_0001 2011_09_26_drive_0002 2011_09_26_drive_0005 2011_09_26_drive_0009 2011_09_26_drive_0011 2011_09_26_drive_0013 2011_09_26_drive_0014 2011_09_26_drive_0015 2011_09_26_drive_0017 2011_09_26_drive_0018 2011_09_26_drive_0019 2011_09_26_drive_0020 2011_09_26_drive_0022 2011_09_26_drive_0023 2011_09_26_drive_0027 2011_09_26_drive_0028 2011_09_26_drive_0029 2011_09_26_drive_0032 2011_09_26_drive_0035 2011_09_26_drive_0036 2011_09_26_drive_0039 2011_09_26_drive_0046 2011_09_26_drive_0048 2011_09_26_drive_0051 2011_09_26_drive_0052 2011_09_26_drive_0056 2011_09_26_drive_0057 2011_09_26_drive_0059 2011_09_26_drive_0060 2011_09_26_drive_0061 2011_09_26_drive_0064 2011_09_26_drive_0070 2011_09_26_drive_0079 2011_09_26_drive_0084 2011_09_26_drive_0086 2011_09_26_drive_0087 2011_09_26_drive_0091 2011_09_26_drive_0093 2011_09_26_drive_0095 2011_09_26_drive_0096 2011_09_26_drive_0101 2011_09_26_drive_0104 2011_09_26_drive_0106 2011_09_26_drive_0113 2011_09_26_drive_0117 2011_09_26_drive_0119 2011_09_26_drive_0121 2011_09_26_drive_0122 2011_09_26_drive_0125 2011_09_26_drive_0126 2011_09_26_drive_0128 2011_09_26_drive_0132 2011_09_26_drive_0134 2011_09_26_drive_0135 2011_09_26_drive_0136 2011_09_26_drive_0138 2011_09_26_drive_0141 2011_09_26_drive_0143 2011_09_26_drive_0145 2011_09_26_drive_0146 2011_09_26_drive_0149 2011_09_26_drive_0153 2011_09_26_drive_0154 2011_09_26_drive_0155 2011_09_26_drive_0156 2011_09_26_drive_0160 2011_09_26_drive_0161 2011_09_26_drive_0162 2011_09_26_drive_0165 2011_09_26_drive_0166 2011_09_26_drive_0167 2011_09_26_drive_0168 2011_09_26_drive_0171\"\n",
        "\n",
        "# Split the string and return the list\n",
        "result_list = strings.split()\n",
        "print(result_list)"
      ],
      "metadata": {
        "id": "JGy0jROGUvyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}